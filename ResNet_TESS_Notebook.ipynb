{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T19:11:21.985874Z","iopub.execute_input":"2025-08-05T19:11:21.986033Z","iopub.status.idle":"2025-08-05T19:11:22.028923Z","shell.execute_reply.started":"2025-08-05T19:11:21.986018Z","shell.execute_reply":"2025-08-05T19:11:22.028424Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# # code to clear up the output folders\n\n# import shutil\n# import os\n\n# working_dir = \"/kaggle/working\"\n\n# for item in os.listdir(working_dir):\n#     item_path = os.path.join(working_dir, item)\n#     try:\n#         if os.path.isdir(item_path):\n#             shutil.rmtree(item_path)\n#         else:\n#             os.remove(item_path)\n#     except Exception as e:\n#         print(f\"Couldn't delete {item_path}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T19:11:22.030185Z","iopub.execute_input":"2025-08-05T19:11:22.030478Z","iopub.status.idle":"2025-08-05T19:11:22.033605Z","shell.execute_reply.started":"2025-08-05T19:11:22.030453Z","shell.execute_reply":"2025-08-05T19:11:22.033077Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Paths\ndataset_dir = \"/kaggle/input/toronto-emotional-speech-set-tess\"\noutput_dir = \"/kaggle/working/spectrograms\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Constants\nTARGET_DURATION = 6  # seconds\nSAMPLE_RATE = 22050  # default librosa sample rate\nTARGET_LEN = SAMPLE_RATE * TARGET_DURATION\n\ndef preprocess_and_save(audio_path, output_path):\n    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n\n    # Normalize\n    y = (y - np.mean(y)) / np.std(y)\n\n    # Trim or pad\n    if len(y) > TARGET_LEN:\n        y = y[:TARGET_LEN]\n    else:\n        y = np.pad(y, (0, TARGET_LEN - len(y)))\n\n    # Mel spectrogram using paper settings\n    S = librosa.feature.melspectrogram(\n        y=y,\n        sr=sr,\n        n_fft=1024,\n        hop_length=int(sr * 0.010),\n        win_length=int(sr * 0.025),\n        window='hamming',\n        n_mels=128\n    )\n    S_DB = librosa.power_to_db(S, ref=np.max)\n\n    # Save as image\n    plt.figure(figsize=(3, 3))\n    librosa.display.specshow(S_DB, sr=sr, x_axis=None, y_axis=None)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n# Go through all .wav files\nfor root, dirs, files in os.walk(dataset_dir):\n    for file in tqdm(files):\n        if file.endswith(\".wav\"):\n            emotion = file.split(\"_\")[-1].replace(\".wav\", \"\")  # Example: happy, angry, etc.\n            emotion_dir = os.path.join(output_dir, emotion)\n            os.makedirs(emotion_dir, exist_ok=True)\n\n            audio_path = os.path.join(root, file)\n            output_path = os.path.join(emotion_dir, file.replace(\".wav\", \".png\"))\n            preprocess_and_save(audio_path, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T19:32:22.354197Z","iopub.execute_input":"2025-08-05T19:32:22.354502Z","execution_failed":"2025-08-05T19:34:49.692Z"}},"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]\n0it [00:00, ?it/s]\n100%|██████████| 200/200 [00:27<00:00,  7.39it/s]\n100%|██████████| 200/200 [00:27<00:00,  7.33it/s]\n 64%|██████▎   | 127/200 [00:18<00:22,  3.27it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimg = Image.open(\"/kaggle/working/spectrograms/angry/OAF_back_angry.png\")\nprint(img.mode)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Transform for ResNet\ntransform = transforms.Compose([\n    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # convert RGBA to RGB\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\n# Load dataset\ndataset = datasets.ImageFolder(\"/kaggle/working/spectrograms\", transform=transform)\n\nfrom torch.utils.data import random_split\n\n# Calculate sizes\ntotal_size = len(dataset)\ntrain_size = int(0.7 * total_size)\nval_size = int(0.15 * total_size)\ntest_size = total_size - train_size - val_size\n\n# Split the dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# Loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18, ResNet18_Weights\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nweights = ResNet18_Weights.DEFAULT\nmodel = resnet18(weights=weights)\n\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final FC layer first\nnum_classes = len(dataset.classes)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Unfreeze last 2 blocks + new fc layer\nfor name, param in model.named_parameters():\n    if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n        param.requires_grad = True\n\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Storing Training Stats\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\nscores = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\ndef Train_model(model, e, fold_no, train_loader, val_loader, optimizer, loss_func):\n    counter = 0\n    patience = 7\n    best_score = 0\n\n    for epoch in range(e):\n        model.train()\n        total_loss = 0\n        train_total = 0\n        train_correct = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = loss_func(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, predicted = torch.max(outputs, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n            total_loss += loss.item()\n\n        train_accuracy = train_correct / train_total * 100\n        train_losses.append(total_loss)\n        train_accuracies.append(train_accuracy)\n\n        # Validation\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        val_total_loss = 0\n        pred_classes = []\n        actual_classes = []\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = loss_func(outputs, labels)\n                val_total_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                pred_classes.extend(predicted.cpu().numpy())\n                actual_classes.extend(labels.cpu().numpy())\n\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_accuracy = val_correct / val_total * 100\n        precision = precision_score(actual_classes, pred_classes, average='weighted')\n        recall = recall_score(actual_classes, pred_classes, average='weighted')\n        f1 = f1_score(actual_classes, pred_classes, average='weighted')\n\n        val_losses.append(val_total_loss)\n        val_accuracies.append(val_accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n\n        print(f\"Epoch: {epoch+1}, Train Loss: {total_loss:.2f}, \"\n              f\"Train Acc: {train_accuracy:.2f}%, Val Loss: {val_total_loss:.2f}, \"\n              f\"Val Acc: {val_accuracy:.2f}%\")\n        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n\n        current_score = (f1 + val_accuracy / 100) / 2\n\n        if current_score > best_score:\n            best_score = current_score\n            scores.append(best_score)\n            counter = 0\n            torch.save(model.state_dict(), f\"best_model_{fold_no}.pth\")\n        else:\n            counter += 1\n            if counter >= patience:\n                print(\"Early stopping triggered\")\n                break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 50\nfold_no = 1  \n\nTrain_model(model, num_epochs, fold_no, train_loader, val_loader, optimizer, loss_func)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(f\"best_model_{fold_no}.pth\"))\nmodel.to(device)\nmodel.eval()\n\n# Evaluate on validation set\nval_correct = 0\nval_total = 0\npred_classes = []\nactual_classes = []\n\nwith torch.no_grad():\n    for images, labels in test_loader: \n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        pred_classes.extend(predicted.cpu().numpy())\n        actual_classes.extend(labels.cpu().numpy())\n\n        val_total += labels.size(0)\n        val_correct += (predicted == labels).sum().item()\n\naccuracy = val_correct / val_total * 100\nprecision = precision_score(actual_classes, pred_classes, average='weighted')\nrecall = recall_score(actual_classes, pred_classes, average='weighted')\nf1 = f1_score(actual_classes, pred_classes, average='weighted')\n\nprint(f\"Final Model Stats:\")\nprint(f\"Accuracy: {accuracy:.2f}%\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(actual_classes, pred_classes)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # downloading the best model\n# from IPython.display import FileLink\n# FileLink(r'best_model_1.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation loss\nplt.figure(figsize=(8, 5))\nplt.plot(train_losses, label=\"Train Loss\", marker='o')\nplt.plot(val_losses, label=\"Validation Loss\", marker='o')\nplt.title(\"Training vs Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training & validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(train_accuracies, label=\"Train Accuracy\", marker='o')\nplt.plot(val_accuracies, label=\"Validation Accuracy\", marker='o')\nplt.title(\"Training vs Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport shutil\n\n# Paths\nravdess_dir = \"/kaggle/input/ravdess-emotional-speech-audio\"\noutput_dir = \"/kaggle/working/testSet\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Emotion code mapping\nlabel_map = {\n    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n    '05': 'angry', '06': 'fear', '07': 'disgust', '08': 'ps'\n}\nExclude_labels = ['02']\n\n# Collect all files with emotion labels\nall_files = []\nfor root, _, files in os.walk(ravdess_dir):\n    for f in files:\n        if f.endswith(\".wav\"):\n            parts = f.split('-')\n            emotion_code = parts[2]\n            if emotion_code in label_map and emotion_code not in Exclude_labels:\n                all_files.append({\n                    'path': os.path.join(root, f),\n                    'emotion': label_map[emotion_code]\n                })\n\n# Shuffle for randomness\nrandom.shuffle(all_files)\n\n# Calculate how many per emotion to total ~200\nunique_emotions = [label_map[code] for code in label_map if code not in Exclude_labels]\nper_emotion = 200 // len(unique_emotions)\n\n# Select files\nselected_files = []\nfor emotion in unique_emotions:\n    emotion_files = [f for f in all_files if f['emotion'] == emotion]\n    selected_files.extend(random.sample(emotion_files, min(per_emotion, len(emotion_files))))\n\n# If we still need more files due to rounding\nwhile len(selected_files) < 200:\n    extra = random.choice(all_files)\n    if extra not in selected_files:\n        selected_files.append(extra)\n\n# Copy to output folder inside class folders\nfor item in selected_files:\n    emotion_folder = os.path.join(output_dir, item['emotion'])\n    os.makedirs(emotion_folder, exist_ok=True)\n    shutil.copy(item['path'], os.path.join(emotion_folder, os.path.basename(item['path'])))\n\nprint(f\"✅ Created dataset with {len(selected_files)} files in {len(unique_emotions)} folders at {output_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\ndataset_dir = \"/kaggle/working/testSet\"\noutput_dir = \"/kaggle/working/TestSet_spectrograms\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Go through all .wav files\nfor root, dirs, files in os.walk(dataset_dir):\n    for file in tqdm(files):\n        if file.endswith(\".wav\"):\n            emotion = os.path.basename(root)\n            emotion_dir = os.path.join(output_dir, emotion)\n            os.makedirs(emotion_dir, exist_ok=True)\n\n            audio_path = os.path.join(root, file)\n            output_path = os.path.join(emotion_dir, file.replace(\".wav\", \".png\"))\n            preprocess_and_save(audio_path, output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# import os\n# output_dir = \"/kaggle/working/testSet\"\n# if os.path.exists(output_dir):\n#     shutil.rmtree(output_dir)  # deletes folder and all contents\n# os.makedirs(output_dir, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outsideTestSet = datasets.ImageFolder(\"/kaggle/working/TestSet_spectrograms\", transform=transform)\noutsideTestSetLoader = DataLoader(outsideTestSet, batch_size=32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(f\"best_model_{fold_no}.pth\"))\nmodel.to(device)\nmodel.eval()\n\n# Evaluate on validation set\nval_correct = 0\nval_total = 0\npred_classes = []\nactual_classes = []\n\nwith torch.no_grad():\n    for images, labels in outsideTestSetLoader: \n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        pred_classes.extend(predicted.cpu().numpy())\n        actual_classes.extend(labels.cpu().numpy())\n\n        val_total += labels.size(0)\n        val_correct += (predicted == labels).sum().item()\n\naccuracy = val_correct / val_total * 100\nprecision = precision_score(actual_classes, pred_classes, average='weighted')\nrecall = recall_score(actual_classes, pred_classes, average='weighted')\nf1 = f1_score(actual_classes, pred_classes, average='weighted')\n\nprint(f\"Final Model Stats:\")\nprint(f\"Accuracy: {accuracy:.2f}%\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(actual_classes, pred_classes)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}